import time
import csv
import os
from typing import List, Dict
from dotenv import load_dotenv
from Tools.llm_factory import get_llm
from agent import app  # Import the LangGraph app
from langchain_core.messages import HumanMessage

load_dotenv()

# --- 1. DEFINE TEST CASES ---
# 10 Questions covering different intents
TEST_CASES = [
    # Group 1: Flight Search (3)
    "Find flights from IAX to LAX",
    "Show me details for flight 1878",
    "Find flights arriving at LAX",

    # Group 2: Delay Analysis (3)
    "How are the delays for flights from IAX to LAX?",
    "Which flights from IAX have high delays?",
    "Are Boeing 737s usually late?",

    # Group 3: Passenger Profiling (2)
    "Do Gold members complain more?",
    "Show history for passenger with record locator ABC123",

    # Group 4: Mixed/Complex (2)
    "What are the best food ratings for flights to LAX?",
    "Find flights from IAX with delays over 15 minutes"
]

MODELS = ["Gemini Flash", "Mistral-7B", "Zephyr-7B"]

# --- COST CONSTANTS (Approximate) ---
# Gemini Flash 1.5 Pricing (Example): $0.075 / 1M input tokens, $0.30 / 1M output tokens
# Simplified here based on characters for Gemini, $0 for open source
GEMINI_INPUT_COST_PER_1K_CHARS = 0.0000185 
GEMINI_OUTPUT_COST_PER_1K_CHARS = 0.000075 # Approx 4x input

def estimate_tokens(text: str) -> int:
    """Simple estimation: 1 token ~= 4 chars"""
    if not text: return 0
    return len(text) // 4

def calculate_cost(model_name: str, input_text: str, output_text: str) -> float:
    if model_name != "Gemini Flash":
        return 0.0
    
    input_chars = len(input_text)
    output_chars = len(output_text)
    
    cost = (input_chars / 1000 * GEMINI_INPUT_COST_PER_1K_CHARS) + \
           (output_chars / 1000 * GEMINI_OUTPUT_COST_PER_1K_CHARS)
    return round(cost, 6)

def run_evaluation():
    print("=== STARTING MODEL EVALUATION ===")
    results = []

    for i, question in enumerate(TEST_CASES, 1):
        print(f"\nProcessing Q{i}: {question}")
        
        # We need to run the retrieval ONCE per question to get the context,
        # but the agent is set up to do everything. 
        # To strictly compare the LLM generation layer, we could extract context first.
        # However, reusing the agent is easier, though retrieval time is included.
        # To measure JUST LLM latency, we need to modify how we invoke or just measure total time.
        # The prompt asks for "Latency taken for the LLM to generate".
        # We can hook into the agent or just measure the total agent time (e.g. End-to-End).
        # Given the "Synthesis" node logic is internal, we will approximate by running the agent.
        
        # To be more precise as per requirements, we will run the Agent but try to isolate LLM.
        # Since we can't easily isolate without refactoring agent, we will measure End-to-End Latency
        # which acts as a proxy, OR we simulate the last step.
        
        # Let's rely on the Agent's "Hybrid" mode.
        
        for model_name in MODELS:
            print(f"  > Testing {model_name}...", end="", flush=True)
            
            inputs = {
                "user_query": question,
                "retrieval_mode": "hybrid",
                "selected_model": model_name
            }
            
            start_time = time.time()
            try:
                # Invoke Agent
                output = app.invoke(inputs)
                end_time = time.time()
                
                latency = round(end_time - start_time, 2)
                final_answer = output.get("final_answer", "")
                
                # Context Check
                has_context = "No"
                if output.get("cypher_results") or (output.get("vector_docs") and output["vector_docs"] != ["Error: Database not loaded."]):
                    has_context = "Yes"
                
                # Metrics
                # Input context is hard to grab exactly without inspecting internal state logs,
                # but we can approximate it from the answer or just use query+answer for now?
                # Better: The agent state has the retrieved docs. We can reconstruct context size.
                
                structured_data = str(output.get("cypher_results", ""))
                unstructured_data = "\n".join(output.get("vector_docs", []))
                context_blob = structured_data + unstructured_data
                
                # Total input = System Prompt + Context + Query
                # We'll just sum context + query for estimation
                full_input_text = context_blob + question
                
                token_count = estimate_tokens(full_input_text) + estimate_tokens(final_answer)
                cost = calculate_cost(model_name, full_input_text, final_answer)
                
                results.append({
                    "Question": question,
                    "Model": model_name,
                    "Answer": final_answer.replace("\n", " "), # Flatten for CSV
                    "Latency (s)": latency,
                    "Token Count": token_count,
                    "Est. Cost": f"${cost:.6f}",
                    "Context Retrieved (Yes/No)": has_context,
                    "Human_Score_Quality (Empty)": "",
                    "Human_Score_Relevance (Empty)": ""
                })
                print(f" Done ({latency}s)")
                
            except Exception as e:
                print(f" Error: {e}")
                results.append({
                    "Question": question,
                    "Model": model_name,
                    "Answer": f"ERROR: {e}",
                    "Latency (s)": 0,
                    "Token Count": 0,
                    "Est. Cost": 0,
                    "Context Retrieved (Yes/No)": "No",
                    "Human_Score_Quality (Empty)": "",
                    "Human_Score_Relevance (Empty)": ""
                })

    # --- SAVE TO CSV ---
    csv_filename = "model_comparison_results.csv"
    keys = results[0].keys()
    
    with open(csv_filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=keys)
        writer.writeheader()
        writer.writerows(results)
        
    print(f"\nâœ… Evaluation Complete! Results saved to {csv_filename}")

if __name__ == "__main__":
    run_evaluation()

