{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART A: CLEANING Customer_comment.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-20T18:29:32.978404Z",
     "iopub.status.busy": "2025-10-20T18:29:32.977994Z",
     "iopub.status.idle": "2025-10-20T18:29:33.437072Z",
     "shell.execute_reply": "2025-10-20T18:29:33.435914Z",
     "shell.execute_reply.started": "2025-10-20T18:29:32.978375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load and Inspect Data\n",
    "\n",
    "First, we load the Customer_comment.csv dataset. We then use .info() and .isnull().sum() to get an initial understanding of the data types, non-null counts, and identify columns with missing values. This helps us form a strategy for cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T18:29:33.438979Z",
     "iopub.status.busy": "2025-10-20T18:29:33.438473Z",
     "iopub.status.idle": "2025-10-20T18:29:33.559282Z",
     "shell.execute_reply": "2025-10-20T18:29:33.55823Z",
     "shell.execute_reply.started": "2025-10-20T18:29:33.438955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_comments = pd.read_csv('Raw_Dataset\\Customer_comment.csv')\n",
    "print(\"Customer_comment.csv loaded successfully!\")\n",
    "print(f\"Original shape: {df_comments.shape}\")\n",
    "df_comments.info()\n",
    "print(df_comments.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Handle Duplicate Rows\n",
    "\n",
    "We check for and remove duplicate rows using df.drop_duplicates(). This is a crucial step to prevent data skew and ensure that our analysis and any potential models are not biased by redundant entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T18:29:33.560727Z",
     "iopub.status.busy": "2025-10-20T18:29:33.560389Z",
     "iopub.status.idle": "2025-10-20T18:29:33.591447Z",
     "shell.execute_reply": "2025-10-20T18:29:33.589589Z",
     "shell.execute_reply.started": "2025-10-20T18:29:33.560696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "initial_rows = len(df_comments)\n",
    "df_comments.drop_duplicates(inplace=True)\n",
    "print(f\"Removed {initial_rows - len(df_comments)} duplicate rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Impute Missing Values\n",
    "\n",
    "Our inspection revealed missing values in several columns. We will apply the following imputation strategies:\n",
    "\n",
    "* **Text Columns (verbatim_text, etc.):** We fill NaN values with the string 'No Comment'. This is better than dropping rows, as it preserves the record and treats the absence of a comment as a distinct piece of information.\n",
    "* **Categorical Columns (arrival_delay_group, etc.):** We fill NaN values with the mode (the most frequent value) of each column. This is a standard imputation technique that maintains the original distribution of the categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T18:29:33.594354Z",
     "iopub.status.busy": "2025-10-20T18:29:33.593945Z",
     "iopub.status.idle": "2025-10-20T18:29:33.618859Z",
     "shell.execute_reply": "2025-10-20T18:29:33.61789Z",
     "shell.execute_reply.started": "2025-10-20T18:29:33.594322Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Fill missing text data\n",
    "text_cols = ['verbatim_text', 'ques_verbatim_text', 'transformed_text', 'sentiments']\n",
    "for col in text_cols:\n",
    "    if col in df_comments.columns:\n",
    "        df_comments[col] = df_comments[col].fillna('N/A')\n",
    "        print(f\"Filled missing values in '{col}' with 'N/A'.\")\n",
    "\n",
    "# Fill missing categorical data\n",
    "categorical_cols = [\n",
    "    'arrival_delay_group', 'departure_delay_group', \n",
    "    'loyalty_program_level', 'fleet_type_description', \n",
    "    'entity', 'response_group', 'seat_factor_band'\n",
    "]\n",
    "for col in categorical_cols:\n",
    "    if col in df_comments.columns and df_comments[col].isnull().any():\n",
    "        mode_value = df_comments[col].mode()[0]\n",
    "        df_comments[col] = df_comments[col].fillna(mode_value)\n",
    "        print(f\"Filled missing values in '{col}' with its mode ('{mode_value}').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Save The Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-20T18:29:33.6208Z",
     "iopub.status.busy": "2025-10-20T18:29:33.620269Z",
     "iopub.status.idle": "2025-10-20T18:29:33.780251Z",
     "shell.execute_reply": "2025-10-20T18:29:33.77927Z",
     "shell.execute_reply.started": "2025-10-20T18:29:33.620772Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_path = 'Customer_comment_cleaned.csv'\n",
    "df_comments.to_csv(output_path, index=False)\n",
    "print(f\"\\nCleaned dataset saved successfully to '{output_path}'!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3475962,
     "sourceId": 6727279,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8501024,
     "sourceId": 13396293,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
